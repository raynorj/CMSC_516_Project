{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hi Dr. McInnes\n",
    "#I was able to make a one-click notebook. However, the output is buried in the\n",
    "#code, so you may find it easier to just go through the notebook using \"Run\".\n",
    "\n",
    "#With that said, we didn't have a tightly-coordinated development process, so\n",
    "#in the interest of representing our respective contribution authentically,\n",
    "#I've minimized the number of modifications made to our code. I hope this\n",
    "#helps explain any haphazardness evident from a design perspective.\n",
    "\n",
    "#Also, I found two bugs while assembling this notebook. I have an idea of how\n",
    "#to fix them, but, again, in the interest of authenticity, I've left them\n",
    "#unmodified (a softmax layer in the sigmoid MLP, and a bug in model_test())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "def get_scores():\n",
    "    os.system(\"python eval/evaluate.py eval/input eval/output\")\n",
    "    fh = open(\"eval/output/scores.txt\")\n",
    "    for line in fh.readlines():\n",
    "        line = line.split(':')\n",
    "        descriptor = line[0].replace(\"score\", \"match\")\n",
    "        if descriptor == \"match\":\n",
    "            descriptor = \"match avg\"\n",
    "        score = line[1]\n",
    "        print(descriptor+\": \"+score)\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Data Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Vasanthi tarigopula\n",
    "\n",
    "f = open(\"train.txt\", \"r\")\n",
    "lines=f.readlines()\n",
    "\n",
    "sent=[[] for i in lines]                    ### initializing list\n",
    "probs=[[] for i in lines]                   ### Intializing list \n",
    "ids = [[] for i in lines]\n",
    "\n",
    "j=0\n",
    "word=''\n",
    "prob=''\n",
    "for i in range(0,len(lines)):               ### loop on each line of file\n",
    "    cols=lines[i].strip().split('\\t')         ### spliting line on the basis of tab\n",
    "    if (len(cols)>2):                         ### if line has data\n",
    "        sent[j].append(cols[1].lower())                 ### Appending word in list named sent\n",
    "        probs[j].append(cols[4])                ### Appending probs in list named probs\n",
    "        ids[j].append(cols[0])\n",
    "    else:\n",
    "        j=j+1\n",
    "        \n",
    "sent = [x for x in sent if x != []]\n",
    "probs = [x for x in probs if x != []]\n",
    "ids = [x for x in ids if x != []]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Training Word2Vec to learn embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Vasanthi Tarigopula\n",
    "# derived from https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XZyzt5NKjOQ\n",
    "max_features = 150\n",
    "max_words = 38\n",
    "# instantiating Word2Vec to train the model by passing the list of lists\n",
    "# build vocabulary and train model\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    sent,\n",
    "    size=max_features,\n",
    "    window=10,\n",
    "    min_count=1,\n",
    "    workers=10,\n",
    "    iter=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function to find embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Vasanthi Tarigopula\n",
    "'''  *******************************************************************\n",
    "       Returns Combined Embeddings of Title and Description of an Issue\n",
    "     *******************************************************************  '''\n",
    "\n",
    "def get_sentence_embeddings(sentence_tokens):\n",
    "  \n",
    "    #to store embeddings of all the words in the sentence    \n",
    "    sent_embeddings= np.zeros([max_words,max_features ])\n",
    "    i = 0\n",
    "    #iterating over all the words in the title one by one\n",
    "    for word in sentence_tokens:\n",
    "        \n",
    "        sent_embeddings[i] = np.array(model[word])  #embedding of a single word\n",
    "        i=i+1\n",
    "        \n",
    "    if len(sentence_tokens) < max_words:\n",
    "      for i in range(len(sentence_tokens), max_words):\n",
    "        sent_embeddings[i] = np.zeros(max_features)\n",
    "        i = i +1\n",
    "         \n",
    "    #mean embedding of all the words in title      \n",
    "    #mean_embedings = np.mean(np.array(sent_embeddings),axis=0)  \n",
    "    \n",
    "    return np.array(sent_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Vasanthi Tarigopula\n",
    "data={}\n",
    "X = []\n",
    "Y= []\n",
    "for i in range(len(sent)):\n",
    "    \n",
    "    sentence_embeddings = get_sentence_embeddings(sent[i])\n",
    "    #print ('Sent[0]',sentence_embeddings[0])\n",
    "    tmp = []\n",
    "    for prob in probs[i]:\n",
    "      tmp.append(float(prob))  \n",
    "    if len(tmp) < 38:\n",
    "      for j in range(len(tmp), 38):\n",
    "        tmp.append(float(0))\n",
    "    X.append(sentence_embeddings)\n",
    "    Y.append(np.array(tmp))\n",
    "   \n",
    "    data[i]={\"IDs\":ids[i], \"sentence\":sent[i], \"embedding\":sentence_embeddings, \"probs\":probs[i]}  #initializing empty list at current dictionary index \n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Declaring the BiLSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author : Vasanthi tarigopula\n",
    "# derived from https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding,Bidirectional\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = 150\n",
    "SEQUENCE_LEN = max_words\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Bidirectional(LSTM(128), input_shape=(SEQUENCE_LEN, max_features)))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(max_words, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['mae'])\n",
    "\n",
    "print(lstm_model.summary())\n",
    "\n",
    "lstm_model.fit(X[:1650,:,:],Y[:1650,:], batch_size=10, epochs=10)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outputting results for model comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/RiTUAL-UH/SemEval2020_Task10_Emphasis_Selection/blob/master/try_it/How_to_Read_data_and_Write_results.py\n",
    "\n",
    "def write_results(word_id_lsts, words_lsts, e_freq_lsts, write_to):\n",
    "    \"\"\"\n",
    "    This function writes results in the format.\n",
    "    :param word_id_lsts: list of word_ids\n",
    "    :param words_lsts: list of words\n",
    "    :param e_freq_lsts: lists of emphasis probabilities\n",
    "    :param write_to: writing directory\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    with open(write_to, 'w') as out:\n",
    "        sentence_id=\"\"\n",
    "        # a loop on sentences:\n",
    "        for i in range(len(words_lsts)):\n",
    "            # a loop on words in a sentence:\n",
    "            for j in range(len(words_lsts[i])):\n",
    "                # writing:\n",
    "                if sentence_id ==i:\n",
    "                    to_write = \"{}\\t{}\\t{}\\t\".format(word_id_lsts[i][j], words_lsts[i][j], e_freq_lsts[i][j])\n",
    "                    out.write(to_write + \"\\n\")\n",
    "                else:\n",
    "                    out.write(\"\\n\")\n",
    "                    to_write = \"{}\\t{}\\t{}\\t\".format(word_id_lsts[i][j], words_lsts[i][j], e_freq_lsts[i][j])\n",
    "                    out.write(to_write + \"\\n\")\n",
    "                    sentence_id = i\n",
    "        out.write(\"\\n\")\n",
    "        out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "word_id_lsts = []\n",
    "words_lsts = []\n",
    "e_freq_lsts = []\n",
    "\n",
    "for i in range(1650,len(data)):\n",
    "    predicted_probs = lstm_model.predict(X[i:i+1,:,:])[0]\n",
    "    word_id_lsts.append(data[i]['IDs'])\n",
    "    words_lsts.append(data[i]['sentence'])\n",
    "    e_freq_lsts.append(predicted_probs)\n",
    "    \n",
    "write_results(word_id_lsts, words_lsts, e_freq_lsts, 'eval/input/res/submission.txt')\n",
    "\n",
    "print(\"BiLSTM model\")\n",
    "get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Start CNN & MLP code****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "from gensim.models import KeyedVectors\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "#adapted from Vasanthi's code\n",
    "def data_loader(filename):\n",
    "    f = open(filename, \"r\")\n",
    "    lines=f.readlines()\n",
    "\n",
    "    sent=[]                    ### initializing list\n",
    "    probs=[]                   ### Intializing list \n",
    "    ids=[]\n",
    "    data = []\n",
    "    \n",
    "    for i in range(0,len(lines)):               ### loop on each line of file\n",
    "        cols=lines[i].strip().split('\\t')         ### spliting line on the basis of tab\n",
    "        if (len(cols)>2):                         ### if line has data\n",
    "            sent.append(cols[1].lower())                 ### Appending word in list named sent\n",
    "            probs.append(float(cols[4]))                ### Appending probs in list named probs\n",
    "            ids.append(cols[0])\n",
    "        else:\n",
    "            if len(sent) > 0:\n",
    "                data.append(DataItem(sent,probs,ids))\n",
    "            sent = []\n",
    "            probs = []\n",
    "            ids = []\n",
    "\n",
    "    return data\n",
    "\n",
    "#Jesse\n",
    "def get_max_length(dataset):\n",
    "    longest = 0\n",
    "    \n",
    "    for item in dataset:\n",
    "        if len(item.word_seq) > longest:\n",
    "            longest = len(item.word_seq)\n",
    "            \n",
    "    return longest\n",
    "\n",
    "#Jesse\n",
    "#def two_split(dataset, split_point):\n",
    "    #temp = dataset[:]\n",
    "    #random.shuffle(temp)\n",
    "    #return temp[0:int(len(temp)*split_point)], temp[int(len(temp)*split_point):]\n",
    "\n",
    "#Jesse\n",
    "def two_split(dataset, split_point):\n",
    "    temp = dataset\n",
    "    return temp[:1650], temp[1650:]\n",
    "\n",
    "#Jesse\n",
    "class Sentence2Matrix:\n",
    "    def __init__(self, filename):\n",
    "        self.word_model = KeyedVectors.load(filename, mmap='r')\n",
    "        \n",
    "    def __getitem__(self, word_seq):\n",
    "            temp = [list(self.word_model[word]) for word in word_seq]\n",
    "            return temp\n",
    "\n",
    "#Jesse\n",
    "class DataItem:\n",
    "    def __init__(self, word_seq, prob_seq, id_seq):\n",
    "        self.word_seq = word_seq\n",
    "        self.prob_seq = prob_seq\n",
    "        self.id_seq = id_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "dataset = data_loader(\"train.txt\")\n",
    "max_length = get_max_length(dataset)\n",
    "train_set, test_set = two_split(dataset, 0.6)\n",
    "\n",
    "mtrxr = Sentence2Matrix(\"w2v.vec\")\n",
    "train_x = [mtrxr[i.word_seq] for i in train_set]\n",
    "train_y = [i.prob_seq for i in train_set]\n",
    "\n",
    "test_x = [mtrxr[i.word_seq] for i in test_set]\n",
    "test_y = [i.prob_seq for i in test_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pad input data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "def pad_input(matrix, max_length):\n",
    "    embedding_length = len(matrix[0])\n",
    "    empty_vector = [0.0 for i in range(embedding_length)]\n",
    "\n",
    "    if len(matrix) < max_length:\n",
    "        for i in range(max_length-len(matrix)):\n",
    "            matrix.append(empty_vector[:])\n",
    "            \n",
    "    return matrix\n",
    "    \n",
    "train_x = np.array([np.array([pad_input(i, max_length)]) for i in train_x])\n",
    "test_x = np.array([np.array([pad_input(i, max_length)]) for i in test_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pad output data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "def pad_output(prob_seq, max_length):\n",
    "    if len(prob_seq) < max_length:\n",
    "        for i in range(max_length-len(prob_seq)):\n",
    "            prob_seq.append(0.0)\n",
    "    return prob_seq\n",
    "            \n",
    "train_y = np.array([np.array(pad_output(i, max_length)) for i in train_y])\n",
    "test_y = np.array([np.array(pad_output(i, max_length)) for i in test_y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define model testing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "def model_test(model, test_items):\n",
    "    word_id_lsts = []\n",
    "    words_lsts = []\n",
    "    e_freq_lsts = []\n",
    "    predicted_probs = model.predict(test_items)\n",
    "\n",
    "    for i in range(len(test_items)):\n",
    "        data_item = test_set[i]\n",
    "\n",
    "        word_id_lsts.append(data_item.id_seq)\n",
    "        words_lsts.append(data_item.word_seq)\n",
    "        e_freq_lsts.append(predicted_probs[i])\n",
    "\n",
    "    write_results(word_id_lsts, words_lsts, e_freq_lsts, 'eval/input/res/submission.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "#derived from https://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/ and Vasanthi's code\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "feature_length = len(train_x[0][0][0])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input_shape = (img_x, img_y, 1)\n",
    "#kernel_h, kernel_w = self.kernel_size\n",
    "\n",
    "#was having issues due to max pooling 2d layer trying to iterate over what was effectively a 1D convolution\n",
    "#max pooling 1D layer wouldn't work due to formatting issues\n",
    "#might work if I could reshape the (pseudo-1D) \"2D convolution\" into an actual 1D convolution\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,feature_length-1), data_format='channels_first', input_shape=(1, max_length, feature_length)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(100, activation=\"sigmoid\"))\n",
    "model.add(Dense(50, activation=\"sigmoid\"))\n",
    "model.add(Dense(max_length, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", \n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=['mae'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=10, epochs=10)\n",
    "model_test(model, test_x)\n",
    "\n",
    "print(\"Sigmoid CNN\")\n",
    "get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jesse\n",
    "#derived from https://adventuresinmachinelearning.com/keras-tutorial-cnn-11-lines/ and Vasanthi's code\n",
    "feature_length = len(train_x[0][0][0])\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#input_shape = (img_x, img_y, 1)\n",
    "#kernel_h, kernel_w = self.kernel_size\n",
    "\n",
    "#was having issues due to max pooling 2d layer trying to iterate over what was effectively a 1D convolution\n",
    "#max pooling 1D layer wouldn't work due to formatting issues\n",
    "#might work if I could reshape the (pseudo-1D) \"2D convolution\" into an actual 1D convolution\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,feature_length-1), data_format='channels_first', input_shape=(1, max_length, feature_length)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(100, activation=\"sigmoid\"))\n",
    "model.add(Dense(50, activation=\"sigmoid\"))\n",
    "model.add(Dense(max_length, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", \n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=['mae'])\n",
    "\n",
    "model.fit(train_x, train_y, batch_size=10, epochs=10)\n",
    "model_test(model, test_x)\n",
    "print(\"Softmax CNN\")\n",
    "get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original code by Vasanthi, edits by Jesse\n",
    "from keras.layers import Softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, input_dim=38*150, activation='sigmoid'))\n",
    "model.add(Dense(240, activation='sigmoid'))\n",
    "model.add(Dense(38, activation='sigmoid'))\n",
    "model.add(Softmax())\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['MAE'])\n",
    "model.fit(X[:1650,:,:],Y[:1650,:], batch_size=10, epochs=5)\n",
    "\n",
    "word_id_lsts = []\n",
    "words_lsts = []\n",
    "e_freq_lsts = []\n",
    "\n",
    "for i in range(1650,len(data)):\n",
    "    predicted_probs = model.predict(X[i:i+1,:,:])[0]\n",
    "    word_id_lsts.append(data[i]['IDs'])\n",
    "    words_lsts.append(data[i]['sentence'])\n",
    "    e_freq_lsts.append(predicted_probs)\n",
    "    \n",
    "write_results(word_id_lsts, words_lsts, e_freq_lsts, 'eval/input/res/submission.txt')\n",
    "\n",
    "print(\"Sigmoid MLP model\")\n",
    "get_scores()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original code by Vasanthi, edits by Jesse\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, input_dim=38*150, activation='sigmoid'))\n",
    "model.add(Dense(240, activation='sigmoid'))\n",
    "model.add(Dense(38, activation='softmax'))\n",
    "model.add(Softmax())\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['MAE'])\n",
    "\n",
    "model.fit(X[:1650,:,:],Y[:1650,:], batch_size=10, epochs=5)\n",
    "\n",
    "word_id_lsts = []\n",
    "words_lsts = []\n",
    "e_freq_lsts = []\n",
    "\n",
    "for i in range(1650,len(data)):\n",
    "    predicted_probs = model.predict(X[i:i+1,:,:])[0]\n",
    "    word_id_lsts.append(data[i]['IDs'])\n",
    "    words_lsts.append(data[i]['sentence'])\n",
    "    e_freq_lsts.append(predicted_probs)\n",
    "    \n",
    "write_results(word_id_lsts, words_lsts, e_freq_lsts, 'eval/input/res/submission.txt')\n",
    "\n",
    "print(\"Softmax MLP model\")\n",
    "get_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
